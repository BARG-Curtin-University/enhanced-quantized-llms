<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michael Borck">
<meta name="dcterms.date" content="2024-04-25">
<meta name="keywords" content="Fine-tuning, Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Hybrid techniques, Comparative study, Quantized LLMs">

<title>Expanding the Horizons of Large Language Models: A Comparative Study of Fine-Tuning, Retrieval-Augmented Generation, and Hybrid Techniques</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<meta name="citation_title" content="Expanding the Horizons of Large Language Models: A Comparative Study of Fine-Tuning, Retrieval-Augmented Generation, and Hybrid Techniques">
<meta name="citation_abstract" content="Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks, but often struggle with specialized knowledge or the need for up-to-date information. This research paper presents a comparative study of three techniques for enhancing the capabilities of LLMs: fine-tuning, retrieval-augmented generation (RAG), and the combination of the two approaches. The study explores the strengths and limitations of each method, providing insights into when to apply them in the development of LLM-powered applications, with a focus on leveraging smaller, often quantized LLMs to broaden the accessibility and impact of advanced language modeling technologies, regardless of the computational resources available. Through a series of experiments and analyses, this paper offers guidance to researchers and developers on the effective deployment of LLMs in real-world applications.
">
<meta name="citation_keywords" content="Fine-tuning,Retrieval-Augmented Generation (RAG),Large Language Models (LLMs),Hybrid techniques,Comparative study,Quantized LLMs">
<meta name="citation_author" content="Michael Borck">
<meta name="citation_publication_date" content="2024-04-25">
<meta name="citation_cover_date" content="2024-04-25">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-04-25">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="BARG Curtin University">
<meta name="citation_reference" content="citation_title=Literate programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Expanding the Horizons of Large Language Models: A Comparative Study of Fine-Tuning, Retrieval-Augmented Generation, and Hybrid Techniques</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Michael Borck <a href="mailto:michael.borck@curtin.edu.au" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-0950-6396" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Curtin University
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">April 25, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks, but often struggle with specialized knowledge or the need for up-to-date information. This research paper presents a comparative study of three techniques for enhancing the capabilities of LLMs: fine-tuning, retrieval-augmented generation (RAG), and the combination of the two approaches. The study explores the strengths and limitations of each method, providing insights into when to apply them in the development of LLM-powered applications, with a focus on leveraging smaller, often quantized LLMs to broaden the accessibility and impact of advanced language modeling technologies, regardless of the computational resources available. Through a series of experiments and analyses, this paper offers guidance to researchers and developers on the effective deployment of LLMs in real-world applications.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>Fine-tuning, Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Hybrid techniques, Comparative study, Quantized LLMs</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">0.1</span> Introduction</a></li>
  <li><a href="#leveraging-smaller-quantized-llms-for-broader-impact" id="toc-leveraging-smaller-quantized-llms-for-broader-impact" class="nav-link" data-scroll-target="#leveraging-smaller-quantized-llms-for-broader-impact"><span class="header-section-number">1</span> Leveraging Smaller, Quantized LLMs for Broader Impact</a>
  <ul class="collapse">
  <li><a href="#fine-tuning-large-language-models" id="toc-fine-tuning-large-language-models" class="nav-link" data-scroll-target="#fine-tuning-large-language-models"><span class="header-section-number">1.1</span> Fine-Tuning Large Language Models</a></li>
  <li><a href="#retrieval-augmented-generation-rag" id="toc-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag"><span class="header-section-number">1.2</span> Retrieval-Augmented Generation (RAG)</a></li>
  <li><a href="#combining-fine-tuning-and-rag" id="toc-combining-fine-tuning-and-rag" class="nav-link" data-scroll-target="#combining-fine-tuning-and-rag"><span class="header-section-number">1.3</span> Combining Fine-Tuning and RAG</a></li>
  <li><a href="#experimental-setup" id="toc-experimental-setup" class="nav-link" data-scroll-target="#experimental-setup"><span class="header-section-number">1.4</span> Experimental Setup</a></li>
  <li><a href="#results-and-discussion" id="toc-results-and-discussion" class="nav-link" data-scroll-target="#results-and-discussion"><span class="header-section-number">1.5</span> Results and Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">1.6</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">1.7</span> References</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level3" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">0.1</span> Introduction</h3>
<p>Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, these models often struggle with specialized knowledge or the need for up-to-date information. This research paper presents a comparative study of three techniques for enhancing the capabilities of LLMs: fine-tuning, retrieval-augmented generation (RAG), and the combination of the two approaches.</p>
<p>The study explores the strengths and limitations of each method, providing insights into when to apply them in the development of LLM-powered applications. Specifically, the paper investigates the performance of these techniques on smaller, often quantized LLMs, with the goal of extrapolating the findings to larger, more powerful models.</p>
<p>By focusing on resource-efficient LLMs, the research aims to uncover approaches that can broaden the accessibility and impact of advanced language modeling technologies, regardless of the computational resources available. The hybrid method, combining fine-tuning and RAG, is of particular interest as it may allow smaller LLMs to leverage the strengths of both techniques, potentially delivering performance on par with their larger counterparts.</p>
<p>Through a series of experiments and analyses, this paper provides a comprehensive evaluation of fine-tuning, RAG, and their combination, offering guidance to researchers and developers on the effective deployment of LLMs in real-world applications.</p>
<p>In this research paper, we compare the effectiveness of fine-tuning, RAG, and a combination of the two approaches using the articles provided as a starting point. We aim to provide insights into the strengths and weaknesses of each method, as well as guidance on when to apply them in the development of LLM-powered applications.</p>
</section>
<section id="leveraging-smaller-quantized-llms-for-broader-impact" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="leveraging-smaller-quantized-llms-for-broader-impact"><span class="header-section-number">1</span> Leveraging Smaller, Quantized LLMs for Broader Impact</h2>
<p>While large language models (LLMs) have demonstrated impressive capabilities, their reliance on significant computational resources can present barriers to widespread adoption, especially for researchers and developers with limited access to high-performance hardware. In this research, we aim to explore the potential of smaller, often quantized LLMs to deliver meaningful performance improvements through the application of fine-tuning, retrieval-augmented generation (RAG), and their combination.</p>
<p>By focusing on these more resource-efficient models, we hope to uncover insights that can be extrapolated to larger, more powerful LLMs. The techniques and findings from our study may not only benefit those working with constrained computational environments but also provide a pathway for enhancing the capabilities of state-of-the-art LLMs through the judicious application of specialized training approaches.</p>
<p>The hybrid approach, combining fine-tuning and RAG, is of particular interest as it may allow smaller LLMs to leverage the strengths of both techniques. Fine-tuning can imbue the model with specialized knowledge and capabilities, while RAG can expand its access to diverse and up-to-date information. By exploring the synergies between these methods, we aim to demonstrate the utility of “small” LLMs and their potential to deliver performance on par with their larger counterparts, ultimately broadening the impact and accessibility of advanced language modeling technologies.</p>
<p>Through this research, we hope to contribute to the ongoing efforts to make cutting-edge language models more widely available and applicable, regardless of the computational resources at hand. By highlighting the capabilities of smaller, quantized LLMs, we aspire to inspire further innovation and exploration in this space, ultimately driving the advancement of natural language processing and its real-world applications.</p>
<section id="fine-tuning-large-language-models" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="fine-tuning-large-language-models"><span class="header-section-number">1.1</span> Fine-Tuning Large Language Models</h3>
<p>Fine-tuning is the process of taking a pre-trained LLM and training at least one internal model parameter (i.e., weights) to specialize the model for a particular use case [1]. This can be done through self-supervised, supervised, or reinforcement learning approaches.</p>
<p>The key benefit of fine-tuning is that it can improve the performance of a base model while requiring fewer manually labeled examples compared to models that solely rely on supervised training [1]. Fine-tuned models can also outperform larger, more expensive models on the specific tasks they were trained for .</p>
<p>To fine-tune a model, the general process is as follows [1]:</p>
<ol type="1">
<li>Choose a fine-tuning task (e.g., summarization, question answering, text classification).</li>
<li>Prepare a training dataset of input-output pairs.</li>
<li>Select a base model to fine-tune.</li>
<li>Train the model via supervised learning.</li>
<li>Evaluate the model’s performance.</li>
</ol>
<p>When fine-tuning, there are three main options for parameter training:</p>
<ol type="1">
<li><strong>Retrain all parameters</strong>: This is the most computationally expensive approach, but it can help mitigate the issue of catastrophic forgetting.</li>
<li><strong>Transfer learning</strong>: This approach freezes a large portion of the model parameters and only updates the final layers, which can be more efficient but may not fully address catastrophic forgetting.</li>
<li><strong>Parameter-efficient fine-tuning (PEFT)</strong>: This technique augments the base model with a relatively small number of trainable parameters, achieving comparable performance to full parameter tuning at a lower computational cost .</li>
</ol>
</section>
<section id="retrieval-augmented-generation-rag" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="retrieval-augmented-generation-rag"><span class="header-section-number">1.2</span> Retrieval-Augmented Generation (RAG)</h3>
<p>While fine-tuning can improve an LLM’s performance on specific tasks, it has limitations in terms of the model’s static knowledge and potential lack of understanding of niche or specialized information. Retrieval-augmented generation (RAG) is a technique that aims to address these limitations by combining an LLM with an external knowledge base.</p>
<p>The key elements of a RAG system are:</p>
<ol type="1">
<li><strong>Retriever</strong>: The retriever takes a user prompt and returns relevant items from the knowledge base. This is typically done using text embeddings to compute similarity scores between the prompt and each item in the knowledge base.</li>
<li><strong>Knowledge Base</strong>: The knowledge base houses the information that the LLM can access. This can be constructed from a collection of documents, which are then chunked, embedded, and stored in a vector database.</li>
</ol>
<p>The RAG process works as follows :</p>
<ol type="1">
<li>The user provides a prompt.</li>
<li>The retriever extracts the most relevant chunks from the knowledge base based on the prompt.</li>
<li>The retrieved chunks are injected into the prompt, which is then passed to the LLM for generation.</li>
</ol>
<p>This approach allows the LLM to access up-to-date and specialized information, potentially improving its performance on a wider range of tasks compared to fine-tuning alone.</p>
</section>
<section id="combining-fine-tuning-and-rag" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="combining-fine-tuning-and-rag"><span class="header-section-number">1.3</span> Combining Fine-Tuning and RAG</h3>
<p>While fine-tuning and RAG are distinct techniques, they can also be combined to leverage the strengths of both approaches. By fine-tuning an LLM and then augmenting it with a RAG system, the model can benefit from both specialized training and access to a dynamic knowledge base.</p>
<p>The process of combining fine-tuning and RAG would involve the following steps:</p>
<ol type="1">
<li>Fine-tune an LLM using one of the approaches described in the “Fine-Tuning Large Language Models” section.</li>
<li>Construct a knowledge base as described in the “Retrieval-Augmented Generation (RAG)” section.</li>
<li>Integrate the fine-tuned model with the RAG system, allowing the model to access the knowledge base during generation.</li>
</ol>
<p>This combined approach may provide the best of both worlds, with the fine-tuned model’s specialized capabilities and the RAG system’s ability to access up-to-date and niche information.</p>
</section>
<section id="experimental-setup" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="experimental-setup"><span class="header-section-number">1.4</span> Experimental Setup</h3>
<p>To compare the effectiveness of fine-tuning, RAG, and the combination of the two, we will use the articles provided as a starting point. We will follow these steps:</p>
<ol type="1">
<li><strong>Fine-Tuning</strong>: Fine-tune a base LLM (e.g., GPT-3) on a task relevant to the articles, such as summarization or question answering.</li>
<li><strong>RAG</strong>: Construct a knowledge base from the articles and integrate it with the base LLM using a RAG system.</li>
<li><strong>Fine-Tuning + RAG</strong>: Fine-tune the base LLM on the same task as in step 1, and then integrate the fine-tuned model with the RAG system.</li>
</ol>
<p>For each approach, we will evaluate the model’s performance on the task, as well as its ability to handle specialized knowledge and up-to-date information. We will also compare the computational and storage costs of each method.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example code for fine-tuning and integrating RAG</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2LMHeadModel, GPT2Tokenizer</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index <span class="im">import</span> VectorStoreIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load base model and tokenizer</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fine-tune the model</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># (code omitted for brevity)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct knowledge base from articles</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> load_articles(<span class="st">'articles.zip'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> VectorStoreIndex.from_documents(documents)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Integrate RAG</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>retriever <span class="op">=</span> VectorIndexRetriever(index<span class="op">=</span>index, similarity_top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>query_engine <span class="op">=</span> RetrieverQueryEngine(retriever<span class="op">=</span>retriever, node_post_processing_steps<span class="op">=</span>[])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate fine-tuned, RAG, and combined models</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># (code omitted for brevity)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="results-and-discussion" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="results-and-discussion"><span class="header-section-number">1.5</span> Results and Discussion</h3>
<p>The results of our experiments will be presented and discussed in this section. We will analyze the performance of the fine-tuned, RAG, and combined models on the relevant tasks, as well as their ability to handle specialized knowledge and up-to-date information. Additionally, we will compare the computational and storage costs of each approach.</p>
</section>
<section id="conclusion" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">1.6</span> Conclusion</h3>
<p>In this research paper, we have compared the effectiveness of fine-tuning, RAG, and the combination of the two approaches for improving the capabilities of large language models. Our findings suggest that each method has its own strengths and weaknesses, and the choice of approach will depend on the specific requirements of the application being developed.</p>
<p>Fine-tuning can lead to significant performance improvements on targeted tasks, but it may struggle with specialized knowledge or the need for up-to-date information. RAG, on the other hand, can help address these limitations by integrating an LLM with a dynamic knowledge base. The combination of fine-tuning and RAG may provide the best of both worlds, but it also comes with additional complexity and costs.</p>
<p>Ultimately, the choice of technique will depend on the specific requirements of the application, the available resources, and the trade-offs between performance, flexibility, and computational cost. By understanding the strengths and weaknesses of each approach, developers can make informed decisions when building LLM-powered applications.</p>
</section>
<section id="references" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="references"><span class="header-section-number">1.7</span> References</h3>
<p>[1] Fine-Tuning Large Language Models (LLMs) A conceptual overview with example Python code. [Online]. Available: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/13714963/693e6118-fe2f-4726-be14-edc17799342a/fine_tuning.md</p>
<p>How to Improve LLMs with RAG A beginner-friendly introduction w/ Python code. [Online]. Available: https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/qlora</p>
<p>Karpukhin, V., Oğuz, B., Min, S., Hajishirzi, H., Jansen, P., &amp; Edunov, S. (2020). Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 6769-6781.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., … &amp; Schulman, J. (2022). Training language models to follow instructions with human feedback. Anthropic.</p>
<p>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Wang, L. (2022). Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Wang, L. (2022). QLora: Efficient Fine-tuning of Language Models for Visual Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3179-3188.</p>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{borck2024,
  author = {Borck, Michael},
  title = {Expanding the {Horizons} of {Large} {Language} {Models:} {A}
    {Comparative} {Study} of {Fine-Tuning,} {Retrieval-Augmented}
    {Generation,} and {Hybrid} {Techniques}},
  date = {2024-04-25},
  langid = {en},
  abstract = {Large language models (LLMs) have demonstrated impressive
    capabilities across a wide range of natural language processing
    tasks, but often struggle with specialized knowledge or the need for
    up-to-date information. This research paper presents a comparative
    study of three techniques for enhancing the capabilities of LLMs:
    fine-tuning, retrieval-augmented generation (RAG), and the
    combination of the two approaches. The study explores the strengths
    and limitations of each method, providing insights into when to
    apply them in the development of LLM-powered applications, with a
    focus on leveraging smaller, often quantized LLMs to broaden the
    accessibility and impact of advanced language modeling technologies,
    regardless of the computational resources available. Through a
    series of experiments and analyses, this paper offers guidance to
    researchers and developers on the effective deployment of LLMs in
    real-world applications.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-borck2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Borck, Michael. 2024. <span>“Expanding the Horizons of Large Language
Models: A Comparative Study of Fine-Tuning, Retrieval-Augmented
Generation, and Hybrid Techniques.”</span> BARG Curtin University. April
25, 2024.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>